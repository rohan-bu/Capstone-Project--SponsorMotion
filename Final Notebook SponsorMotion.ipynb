{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2275ebb7",
   "metadata": {},
   "source": [
    "## Scalable and Cost-Effective Deduplication: Leveraging Algorithms and LLMs\n",
    "\n",
    "### By: Rohan Chaudhary, Sarmad Kahut, Valentina Torres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86185ff7",
   "metadata": {},
   "source": [
    "The presented Jupyter notebook focuses exclusively on the states of Florida and Nevada as the selected testing states for the deduplication challenge. These states were chosen for their specific characteristics and relevance to the task. However, it's important to note that this is just a subset of the complete set of states. To ensure comprehensive results, the user from SponsorMotion is encouraged to replicate the provided code and model with the remaining states in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae9877",
   "metadata": {},
   "source": [
    "### Data Cleaning, Sorting and Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd6c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad06fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/4pcz1qlx5vg004krks9c5vsm0000gn/T/ipykernel_48445/2455299057.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('230607 Events dump.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48265, 21)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv('ordered.csv')\n",
    "data = pd.read_csv('230607 Events dump.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7868bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna(subset=['name','start date','state'],how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2349c6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45011, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ab986",
   "metadata": {},
   "source": [
    "After dropping all the rows that have no name, no start date and no state, the dataset resulted in 45,011 records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456a313d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                object\n",
       "end date               object\n",
       "flagged_by             object\n",
       "has_embedding          object\n",
       "human verification     object\n",
       "is virtual             object\n",
       "is_flagged             object\n",
       "is_free                object\n",
       "location               object\n",
       "name                   object\n",
       "sourceURL              object\n",
       "specialties            object\n",
       "start date             object\n",
       "state                  object\n",
       "summary                object\n",
       "URL                   float64\n",
       "Creation Date          object\n",
       "Modified Date          object\n",
       "Slug                  float64\n",
       "Creator                object\n",
       "unique id              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232418fc",
   "metadata": {},
   "source": [
    "The following code is used to reformat the \"start date\" column into the YYYY/MM/DD format. This adjustment is crucial as it greatly facilitates data sorting and enables efficient tracking of duplicates. By standardizing the start date representation, it becomes an important criterion for grouping potential duplicates accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9adb5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['start date'] = pd.to_datetime(data['start date'], format='%b %d, %Y %I:%M %p', errors='coerce').dt.strftime('%Y/%m/%d')\n",
    "data['start date'] = pd.to_datetime(data['start date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646e9d1",
   "metadata": {},
   "source": [
    "We are adding a new column that shows the summary length in terms of the number of words. Moreover, we will retain the initial column, which corresponds to the longest summary. By doing this we want to prioritize rows with the most comprehensive event information ensuring that we preserve entries with richer details, enhancing the overall data quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba61878",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['summary_len']=data['summary'].apply(lambda x: len(x) if isinstance(x, str) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb41bf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    469.0\n",
       "2    487.0\n",
       "3    742.0\n",
       "4    247.0\n",
       "5    364.0\n",
       "6    491.0\n",
       "7    207.0\n",
       "8    505.0\n",
       "9    487.0\n",
       "Name: summary_len, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['summary_len'][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7f1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_dataset(data):\n",
    "    data.sort_values(by=['start date', 'state','summary_len'],ascending=[True,True,False] ,inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "ordered_data = order_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58588e8",
   "metadata": {},
   "source": [
    "Below, we are customizing the \"name\" column. Our process involves eliminating all spaces and special characters, followed by converting all text to lowercase. This tailored approach serves the purpose of maintaining a standardized format for the information. By carrying out these modifications, we'll enhance the effectiveness of the fuzzy matching algorithm, enabling it to more efficiently identify potential duplicates. This strategic adjustment contributes to a more precise deduplication process and overall data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a1064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all the initial and ending spaces\n",
    "ordered_data['name']=ordered_data['name'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a337f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing # and / from text\n",
    "ordered_data['name'] = ordered_data['name'].str.replace(r'[#/\\$()-;Ä+¬Æ¬†¬∞Ñ¢ì @!≤√]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7abd8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data['name'] = ordered_data['name'].str.replace(' ', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04cc5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data['name']=ordered_data['name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d019c128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                chickasawconstitutiondrafting\n",
       "2                        thebattleofgettysburg\n",
       "3        feministoratorwowsterritorialcheyenne\n",
       "4    americangynecologicalsocietyannualmeeting\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_data['name'][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925bd17",
   "metadata": {},
   "source": [
    "Now we pick the states for which we want to analyze the information. In this notebook we will use Florida and Nevada which are 2 of the states with the highest number of records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1becebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = ordered_data[ordered_data['state'].isin(['FL','NV'])]# Change the state name accroding to your desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033f9ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1772, 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd1493",
   "metadata": {},
   "source": [
    "### Deduplication Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814aa7b",
   "metadata": {},
   "source": [
    "We will first filter all the records that have same name, start date and state since they are potential duplicates and only keep the first one which is the one that has the longest summary. After the otehr records are dropped, we will end up with a dataset that contains unique events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "096d0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping unique records of names start date and state\n",
    "unique_events= filtered_data.drop_duplicates(['name','start date','state'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dd2dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/4pcz1qlx5vg004krks9c5vsm0000gn/T/ipykernel_48445/4027050285.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_events.drop(columns=['summary_len'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "unique_events.drop(columns=['summary_len'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187ec47",
   "metadata": {},
   "source": [
    "While these records are distinct, some could potentially be duplicates, differing by just +1 or -1 days in their start dates. There's a chance they share identical names coupled with similar start dates, or slightly differing names alongside comparable or even identical start dates. To address this, we'll employ a fuzzy matching algorithm using the name column. This technique will effectively flag potential duplicates, taking into account the subtle variations in names and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddc235e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /Users/valentinatorres/anaconda3/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /Users/valentinatorres/anaconda3/lib/python3.11/site-packages (0.21.1)\n",
      "Requirement already satisfied: Levenshtein==0.21.1 in /Users/valentinatorres/anaconda3/lib/python3.11/site-packages (from python-Levenshtein) (0.21.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /Users/valentinatorres/anaconda3/lib/python3.11/site-packages (from Levenshtein==0.21.1->python-Levenshtein) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425b96a",
   "metadata": {},
   "source": [
    "When we run the algorithm we will use a treshold of 75 which has shown the highest accuracy during testing and less amount of false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa966aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to check fuzzy matching similarity\n",
    "def fuzzy_match_similarity(name_a, name_b):\n",
    "    return fuzz.token_set_ratio(name_a, name_b)\n",
    "\n",
    "# Apply fuzzy matching on all name pairs within each state group\n",
    "potential_duplicates = []\n",
    "\n",
    "# Group by 'state' column\n",
    "grouped_events = unique_events.groupby('state')\n",
    "\n",
    "for group_name, group_data in grouped_events:\n",
    "    # Sort the group_data DataFrame by 'start date'\n",
    "    group_data = group_data.sort_values(by='start date')\n",
    "    \n",
    "    for i in range(len(group_data)):\n",
    "        for j in range(i + 1, len(group_data)):\n",
    "            name_a = group_data.iloc[i]['name']\n",
    "            name_b = group_data.iloc[j]['name']\n",
    "            formatted_date_a = pd.to_datetime(group_data.iloc[i]['start date'])\n",
    "            formatted_date_b = pd.to_datetime(group_data.iloc[j]['start date'])\n",
    "            \n",
    "            # Check if the dates are the same or differ by one day\n",
    "            date_difference = abs((formatted_date_a - formatted_date_b).days)\n",
    "            \n",
    "            if date_difference <= 1:  # Events are either on the same date or differ by one day\n",
    "                similarity = fuzzy_match_similarity(name_a, name_b)\n",
    "                if similarity >= 75:\n",
    "                    potential_duplicates.append((group_data.iloc[i], group_data.iloc[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5629d6",
   "metadata": {},
   "source": [
    "We're currently refining the identification of potential duplicates by introducing a new column titled \"flagged_duplicates_numbers.\" This column assigns a consistent number to all rows that qualify as potential duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bb28d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/4pcz1qlx5vg004krks9c5vsm0000gn/T/ipykernel_48445/2100794347.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_events['flagged_duplicates_numbers'] = 0\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "# Define a function to check fuzzy matching similarity\n",
    "def fuzzy_match_similarity(name_a, name_b):\n",
    "    return fuzz.token_set_ratio(name_a, name_b)\n",
    "# Assuming you have already obtained the 'potential_duplicates' list\n",
    "# Create a list to store sets of duplicate name and start date combinations\n",
    "duplicate_groups = []\n",
    "for pair in potential_duplicates:\n",
    "    row_a, row_b = pair\n",
    "    name_a = row_a['name']\n",
    "    date_a = row_a['start date']\n",
    "    name_b = row_b['name']\n",
    "    date_b = row_b['start date']\n",
    "    # Find if either name or date exists in any existing group\n",
    "    found_group_index = None\n",
    "    for i, group in enumerate(duplicate_groups):\n",
    "        if (name_a, date_a) in group or (name_b, date_b) in group:\n",
    "            found_group_index = i\n",
    "            break\n",
    "    # If both name and date are new, create a new group\n",
    "    if found_group_index is None:\n",
    "        duplicate_groups.append({(name_a, date_a), (name_b, date_b)})\n",
    "    else:\n",
    "        # Add the new name and date to the existing group\n",
    "        duplicate_groups[found_group_index].add((name_a, date_a))\n",
    "        duplicate_groups[found_group_index].add((name_b, date_b))\n",
    "# Create a dictionary to map each unique name and start date combination to a numerical value\n",
    "name_date_to_numerical = {}\n",
    "numerical_value = 1\n",
    "for group in duplicate_groups:\n",
    "    for name, date in group:\n",
    "        name_date_to_numerical[(name, date)] = numerical_value\n",
    "    numerical_value += 1\n",
    "# Create a new column 'flagged_duplicates_numbers' in the original dataset\n",
    "unique_events['flagged_duplicates_numbers'] = 0\n",
    "# Iterate through the rows and set the numerical values for potential duplicates\n",
    "for index, row in unique_events.iterrows():\n",
    "    name = row['name']\n",
    "    date = row['start date']\n",
    "    if (name, date) in name_date_to_numerical:\n",
    "        unique_events.at[index, 'flagged_duplicates_numbers'] = name_date_to_numerical[(name, date)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8ec248",
   "metadata": {},
   "source": [
    "Finally, we add an extra column \"human_verification_needed\" this will indicate if that record needs further verification. It will appear as \"yes\" if the records have similar names but different start dates (+ or - 1 day) and it will appear as \"no\" if the records have similar names and same dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e4272b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/4pcz1qlx5vg004krks9c5vsm0000gn/T/ipykernel_48445/1996621375.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unique_events['human_verification_needed'] = 'No'\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have the DataFrame 'unique_events' with the 'flagged_duplicates_numbers' column\n",
    "# Create a new column 'human_verification_needed' and initialize it with 'No'\n",
    "unique_events['human_verification_needed'] = 'No'\n",
    "# Iterate through the rows and update 'human_verification_needed' based on the conditions\n",
    "for index, row in unique_events.iterrows():\n",
    "    flagged_num = row['flagged_duplicates_numbers']\n",
    "    if flagged_num != 0:\n",
    "        same_group_rows = unique_events[unique_events['flagged_duplicates_numbers'] == flagged_num]\n",
    "        start_date = row['start date']\n",
    "        # Collect unique 'start_date' values for the group\n",
    "        unique_dates = set(same_group_rows['start date'])\n",
    "        unique_dates.remove(start_date)  # Remove the current 'start_date' to avoid checking against itself\n",
    "        # Check if 'start_date' differs by exactly one day for any other row in the same group\n",
    "        for date in unique_dates:\n",
    "            if pd.Timedelta(days=-1) == date - start_date or pd.Timedelta(days=1) == date - start_date:\n",
    "                unique_events.at[index, 'human_verification_needed'] = 'Yes'\n",
    "                break\n",
    "# Print the updated dataset with the new column\n",
    "#print(unique_events)\n",
    "unique_events.to_csv('FL+NV_75.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430139a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
